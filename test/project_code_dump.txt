# === main.py ===
# main.py
import sys
import agent

def main():
    last_results = {} 
    
    while True:
        user_query = input("请输入查询 (输入'退出'结束): ")
        if user_query.lower() in ["退出", "exit", "quit", "q"]:
            print("谢谢使用，再见！")
            break

        if "上次" in user_query or "之前" in user_query or "刚才" in user_query:
            pass
            
        result = agent.chat(user_query)
        last_results = result

        print("\n是否有其他问题？")

if __name__ == '__main__':
    main()


# === agent.py ===
#! /usr/bin/env python3
import re
import json
import datetime
import requests
import time
import os  
import traceback
import hashlib
import config 
import dateparser
from django.conf import settings

from output.report_generator import get_anomaly_detection_report
from analysis.single_series import analyze_single_series
from analysis.multi_series import analyze_multi_series
from output.report_generator import generate_report_single, generate_report_multi
from output.visualization import generate_summary_echarts_html
from utils.ts_cache import ensure_cache_file, load_series_from_cache
from utils.time_utils import parse_time_expressions

AIOPS_BACKEND_DOMAIN = 'https://aiopsbackend.cstcloud.cn'
LLM_URL = 'http://10.16.1.16:58000/v1/chat/completions'
AUTH = ('chelseyyycheng@outlook.com', 'UofV1uwHwhVp9tcTue')

CACHE_DIR = "cached_data"
os.makedirs(CACHE_DIR, exist_ok=True)

def fetch_data_from_backend(ip:str, start_ts:int, end_ts:int, field:str):
    url = f"{AIOPS_BACKEND_DOMAIN}/api/v1/monitor/mail/metric/format-value/?start={start_ts}&end={end_ts}&instance={ip}&field={field}"
    resp = requests.get(url, auth=AUTH)
    if resp.status_code!=200:
        return f"后端请求失败: {resp.status_code} => {resp.text}"
    j = resp.json()
    results = j.get("results", [])
    if not results:
        return []
    vals = results[0].get("values", [])
    arr = []
    from datetime import datetime
    def parse_ts(s):
        try:
            dt = datetime.strptime(s,"%Y-%m-%d %H:%M:%S")
            return int(dt.timestamp())
        except:
            return 0
    for row in vals:
        if len(row)>=2:
            tstr,vstr = row[0], row[1]
            t = parse_ts(tstr)
            try:
                v = float(vstr)
            except:
                v = 0.0
            arr.append([t,v])
    return arr

tools = [
     {
        "name":"解析用户自然语言时间",
        "description":"返回一个list，每个元素是{start, end, error}. 如果不确定，可向用户澄清。",
        "parameters":{
            "type":"object",
            "properties":{
                "raw_text":{"type":"string"}
            },
            "required":["raw_text"]
        }
    },
    {  
        "name": "请求智能运管后端Api，获取指标项的时序数据",
        "description": "从后端或本地缓存获取IP在指定时间范围(field)的时序数据(list of [int_ts, val])。注意start/end必须是形如'YYYY-MM-DD HH:MM:SS'的确定时间。",
        "parameters": {
            "type": "object",
            "properties": {
                "ip": {
                    "type": "string",
                    "description": "要查询的 IP，如 '192.168.0.110'"
                },
                "start": {
                    "type": "string",
                    "description": "开始时间，格式 '2025-03-24 00:00:00'"
                },
                "end": {
                    "type": "string",
                    "description": "结束时间，格式 '2025-03-24 23:59:59'"
                },
                "field": {
                    "type": "string",
                    "description": "监控项名称，如 'cpu_rate'"
                }
            },
            "required": ["ip","start","end","field"]
        }
    },
    {
        "name": "请求智能运管后端Api，查询监控实例有哪些监控项",
        "description": "返回指定IP下可用的监控项列表（可选项）",
        "parameters": {
            "type": "object",
            "properties": {
                "service": {
                    "type": "string",
                    "description": "系统服务名称 (一般填 '主机监控')"
                },
                "instance": {
                    "type": "string",
                    "description": "监控实例 IP"
                }
            },
            "required": ["service","instance"]
        }
    },
    {
        "name": "请求智能运管后端Api，查询监控服务的资产情况和监控实例",
        "description": "查询一个监控服务的所有资产/IP等信息",
        "parameters": {
            "type": "object",
            "properties": {
                "service": {
                    "type": "string",
                    "description": "要查询的系统服务名称"
                }
            },
            "required": ["service"]
        }
    },
    {
        "name": "请求智能运管后端Api，查询监控实例之间的拓扑关联关系",
        "description": "查询指定IP的上联、下联监控实例等信息",
        "parameters": {
            "type": "object",
            "properties": {
                "service": {
                    "type": "string",
                    "description": "系统服务名称"
                },
                "instance_ip": {
                    "type": "string",
                    "description": "监控实例IP"
                }
            },
            "required": ["service","instance_ip"]
        }
    },
    {
        "name": "单序列异常检测(文件)",
        "description": "对单序列 [int_ts,val] 进行多方法分析, 生成报告和ECharts HTML",
        "parameters": {
            "type": "object",
            "properties": {
                "ip":    {"type": "string"},
                "field": {"type": "string"},
                "start": {"type": "string"},
                "end":   {"type": "string"}
            },
            "required": ["ip","field","start","end"]
        }
    },
    {
        "name": "多序列对比异常检测(文件)",
        "description": "对两组 [int_ts,val] 进行对比分析, 生成报告和ECharts HTML",
        "parameters": {
            "type": "object",
            "properties": {
                "ip1":    {"type": "string"},
                "field1": {"type": "string"},
                "start1": {"type": "string"},
                "end1":   {"type": "string"},
                "ip2":    {"type": "string"},
                "field2": {"type": "string"},
                "start2": {"type": "string"},
                "end2":   {"type": "string"}
            },
            "required": ["ip1","field1","start1","end1","ip2","field2","start2","end2"]
        }
    }
]

def monitor_item_list(ip):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/monitor/mail/machine/field/?instance={ip}'
    resp = requests.get(url=url, auth=AUTH)
    if resp.status_code == 200:
        items = json.loads(resp.text)
        result = {}
        for x in items:
            result[x.get('field')] = x.get('purpose')
        return result
    else:
        return f"查询监控项失败: {resp.status_code} => {resp.text}"

def get_service_asset(service):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/property/mail/?ordering=num_id&page=1&page_size=2000'
    resp = requests.get(url=url, auth=AUTH)
    if resp.status_code == 200:
        text = json.loads(resp.text)
        results = text.get('results',[])
        item_list = []
        for r in results:
            r["category"] = r.get("category",{}).get("name")
            r["ip_set"] = [_.get("ip") for _ in r.get('ip_set',[])]
            for k in ["num_id","creation","modification","remark","sort_weight","monitor_status"]:
                r.pop(k, None)
            for k,v in list(r.items()):
                if not v or v == "无":
                    r.pop(k)
            item_list.append(r)
        return item_list
    else:
        return f"查询失败: {resp.status_code} => {resp.text}"

def get_service_asset_edges(service, instance_ip):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/property/mail/topology/search?instance={instance_ip}'
    resp = requests.get(url=url, auth=AUTH)
    if resp.status_code == 200:
        return json.loads(resp.text)
    else:
        return f"查询拓扑失败: {resp.status_code} => {resp.text}"
    
def get_series_data(ip: str, start: str, end: str, field: str):
    try:
        return load_series_from_cache(ip, field, start, end)
    except Exception as e:
        return str(e) 

def validate_multi_series_params(action_input):
    #验证多序列对比异常检测的参数，目前只能处理两个序列的对比

    sequence_nums = set()
    for key in action_input.keys():
        if key.startswith(('ip', 'field', 'start', 'end')) and len(key) > 2 and key[2:].isdigit():
            sequence_nums.add(int(key[2:]))
    
    if len(sequence_nums) > 2 or max(sequence_nums, default=0) > 2:
        return False
    required_pairs = [
        ('ip1', 'field1', 'start1', 'end1'),
        ('ip2', 'field2', 'start2', 'end2')
    ]
    
    for pair in required_pairs:
        if not all(param in action_input for param in pair):
            return False
    return True

###############################################################################
def single_series_detect(ip, field, start, end, user_query=""):

    try:
        series = load_series_from_cache(ip, field, start, end)
        query_text = user_query or f"分析 {ip} 在 {start} ~ {end} 的 {field} 数据"
        
        result = generate_report_single(series, ip, field, query_text)
        result["user_query"] = query_text
        return result
    except Exception as e:
        print(f"单序列分析生成报告失败: {e}")
        traceback.print_exc()
        
        #基本分析作为备选
        try:
            series = load_series_from_cache(ip, field, start, end)
            analysis_result = analyze_single_series(series)
            return {
                "classification": analysis_result["classification"],
                "composite_score": analysis_result["composite_score"],
                "anomaly_times": analysis_result["anomaly_times"],
                "report_path": "N/A - 报告生成失败",
                "user_query": user_query or f"分析 {ip} 在 {start} ~ {end} 的 {field} 数据"
            }
        except:
            return {"error": f"无法加载或处理数据: {str(e)}"}

def multi_series_detect(ip1, field1, start1, end1, ip2, field2, start2, end2, user_query=""):
    try:
        series1 = load_series_from_cache(ip1, field1, start1, end1)
        series2 = load_series_from_cache(ip2, field2, start2, end2)
        
        query_text = user_query or f"对比分析 {ip1} 在 {start1} 和 {ip2} 在 {start2} 的 {field1}/{field2} 指标"

        result = generate_report_multi(series1, series2, ip1, ip2, field1, query_text)
        result["user_query"] = query_text
        return result
    except Exception as e:
        print(f"多序列分析生成报告失败: {e}")
        traceback.print_exc()
        
        try:
            series1 = load_series_from_cache(ip1, field1, start1, end1)
            series2 = load_series_from_cache(ip2, field2, start2, end2)
            analysis_result = analyze_multi_series(series1, series2)
            return {
                "classification": analysis_result["classification"],
                "composite_score": analysis_result["composite_score"],
                "anomaly_times": analysis_result["anomaly_times"],
                "anomaly_intervals": analysis_result.get("anomaly_intervals", []),
                "report_path": "N/A - 报告生成失败",
                "user_query": query_text
            }
        except:
            return {"error": f"无法加载或处理数据: {str(e)}"}

###############################################################################

def llm_call(messages):
    data={
      "model":"Qwen2.5-14B-Instruct",
      "temperature":0.1,
      "messages":messages
    }
    r= requests.post(LLM_URL, json=data)
    if r.status_code==200:
        jj= r.json()
        if "choices" in jj and len(jj["choices"])>0:
            return jj["choices"][0]["message"]
        else:
            return None
    else:
        print("Error:", r.status_code, r.text)
        return None

def parse_llm_response(txt):
    pat_thought = r"<思考过程>(.*?)</思考过程>"
    pat_action  = r"<工具调用>(.*?)</工具调用>"
    pat_inparam = r"<调用参数>(.*?)</调用参数>"
    pat_final   = r"<最终答案>(.*?)</最终答案>"
    pat_supplement = r"<补充请求>(.*?)</补充请求>"
    def ext(pattern):
        m = re.search(pattern, txt, flags=re.S)
        return m.group(1) if m else ""

    return {
        "thought": ext(pat_thought),
        "action":  ext(pat_action),
        "action_input": ext(pat_inparam),
        "final_answer": ext(pat_final),
        "supplement": ext(pat_supplement)
    }

def react(llm_text):
    parsed = parse_llm_response(llm_text)
    action = parsed["action"]
    inp_str = parsed["action_input"]
    final_ans = parsed["final_answer"]
    supplement = parsed["supplement"]
    is_final = False

    if action and inp_str:
        try:
            action_input = json.loads(inp_str)
        except:
            return f"无法解析调用参数JSON: {inp_str}", False

        if action == "解析用户自然语言时间":
            return parse_time_expressions(action_input["raw_text"]), False
        if action == "请求智能运管后端Api，获取指标项的时序数据":
            data = get_series_data(**action_input)
            return data, False
        if action == "请求智能运管后端Api，查询监控实例有哪些监控项":
            return monitor_item_list(action_input["instance"]), False
        elif action == "请求智能运管后端Api，查询监控服务的资产情况和监控实例":
            return get_service_asset(action_input["service"]), False
        elif action == "请求智能运管后端Api，查询监控实例之间的拓扑关联关系":
            return get_service_asset_edges(action_input["service"], action_input["instance_ip"]), False
        
        elif action == "单序列异常检测(文件)":
            result = single_series_detect(**action_input)
            if "error" in result:
                return result["error"], False

            report = get_anomaly_detection_report(result, "single")
            return report, False
        elif action == "多序列对比异常检测(文件)":
            if not validate_multi_series_params(action_input):
                return "参数验证失败，请确保提供了两组完整的序列信息", False
            result = multi_series_detect(**action_input)
            if "error" in result:
                return result["error"], False

            report = get_anomaly_detection_report(result, "multi")
            return report, False
        else:
            return f"未知工具调用: {action}", False
        
    if supplement.strip():
        return {"type": "supplement", "content": supplement}

    if final_ans.strip():
        is_final = True
        return final_ans, is_final
    return ("格式不符合要求，必须使用：<思考过程></思考过程> <工具调用></工具调用> <调用参数></调用参数> <最终答案></最终答案>", is_final)

def shorten_tool_result(res):
    if isinstance(res, list):
        if len(res) > 0 and isinstance(res[0], dict) and "start" in res[0] and "end" in res[0]:
            for item in res:
                if "error" not in item or not item["error"]:
                    if "start_str" not in item:
                        start_dt = datetime.datetime.fromtimestamp(item["start"])
                        item["start_str"] = start_dt.strftime("%Y-%m-%d %H:%M:%S") 
                    if "end_str" not in item:
                        end_dt = datetime.datetime.fromtimestamp(item["end"])
                        item["end_str"] = end_dt.strftime("%Y-%m-%d %H:%M:%S")
            
            #一个简单的的摘要
            time_results = []
            for item in res:
                if "error" in item and item["error"]:
                    time_results.append({"error": item["error"]})
                else:
                    time_results.append({
                        "start_time": item.get("start_str", ""),
                        "end_time": item.get("end_str", ""),
                        "start": item.get("start", 0),
                        "end": item.get("end", 0)
                    })
            return json.dumps(time_results, ensure_ascii=False)
        return f"[List len={len(res)}]"
    elif isinstance(res, dict):
        summary = {}
        for k,v in res.items():
            if isinstance(v, list):
                summary[k] = f"[List len={len(v)}]"
            elif isinstance(v, str) and len(v)>300:
                summary[k] = v[:300] + f"...(omitted, length={len(v)})"
            else:
                summary[k] = v
        return json.dumps(summary, ensure_ascii=False)
    elif isinstance(res, str) and len(res)>300:
        return res[:300] + f"...(omitted, length={len(res)})"
    else:
        return str(res)

def chat(user_query):
    system_prompt = f'''你是一个严格遵守格式规范的用于运维功能，运维数据可视化，运行于生产环境的ReAct智能体，你叫小助手，必须按以下格式处理请求：

    你的工具列表如下:
    {json.dumps(tools, ensure_ascii=False, indent=2)}
    当前时间为: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

    处理规则：
    1.请根据当前时间来推断用户输入的具体日期。
    2.如果用户输入1个时间区间，则调用'单序列异常检测(文件)'。
    3.如果用户输入多个时间区间，但是没有明显的比较词汇，则要在<补充请求>里提问，示例:
    <思考过程>我不知道用户是要对这些时间的数据分别进行单序列分析还是一起多序列分析，我需要确认</思考过程> <工具调用></工具调用> <调用参数></调用参数> <最终答案></最终答案> <补充请求>请问您是想对每段数据进行单序列分析，还是需要多序列的对比分析</补充请求> 
    4.如过用户输入2个时间区间，并且用户输入包含"对比"、"相比"、"比较"、"环比"、"VS"、"vs"、"变化"、"相较于"等明显比较词汇，则调用'多序列对比异常检测(文件)'。
    5.根据用户的输入来判断是否要调用工具以及调用哪个工具,判断不确定的时候可以使用<补充请求>来询问用户
    6.你每次只能调用一个工具，不能在同一次响应中调用多个工具，如果有多个任务，请分轮执行。
    7.不能伪造数据
    8.严格按照以下xml格式生成响应文本：
    ```
    <思考过程>你的思考过程</思考过程>
    <工具调用>工具名称，不调用则为空</工具调用>
    <调用参数>工具输入参数{{json}}</调用参数>
    <最终答案>用户问题的最终结果（知道问题的最终答案时返回）</最终答案>
    <补充请求>系统请求用户补充信息</补充请求>
    ```
    【重要的原则】：
    1."解析用户自然语言时间"工具无法解析时间时，请你先自己根据当前时间和用户语义来计算正确的时间。
    2.当你的工具调用遇到错误时（例如"无效的field"），你必须主动思考如何解决这个问题，而不是立即询问用户。
    例如，如果出现"无效的field"错误，你应该自己主动调用"请求智能运管后端Api，查询监控实例有哪些监控项"工具来查询可用的监控项。
    3.模糊的信息通过<补充请求>来询问用户，明确的信息直接调用工具。
    
    '''
    history=[]
    history.append({"role":"system","content":system_prompt})
    history.append({"role":"user","content": user_query})

    round_num=1
    max_round=15
    pending_context = None 
    had_supplement = False
    # 记录原始用户查询
    original_user_query = user_query

    while True:
        print(f"=== 第{round_num}轮对话 ===")

        if pending_context:
            ans = llm_call(pending_context["history"])
            pending_context = None  
        else:
            ans = llm_call(history)
            
        if not ans:
            print("大模型返回None,结束")
            return

        print(ans["content"])

        history.append(ans)
        txt = ans.get("content","")
        res = react(txt)

        if isinstance(res, dict) and res.get("type") == "supplement":
            print(f"\n小助手: {res['content']}")
            try:
                user_input = input("你: ")
                if not user_input.strip():
                    user_input = "默认继续单序列分析"
            except Exception as e:
                print(f"无法获取用户输入: {e}")
                user_input = "默认继续单序列分析"
            
            supplement_response = f"对于您的问题 '{res['content']}'，我的回答是: {user_input}"
            history.append({"role": "user", "content": supplement_response})
            
            had_supplement = True
            round_num += 1
            continue

        result, done = res
        
        if any(action in txt for action in ["单序列异常检测(文件)", "多序列对比异常检测(文件)"]):
            try:
                action_input_match = re.search(r'<调用参数>(.*?)</调用参数>', txt, re.DOTALL)
                if action_input_match:
                    action_input = json.loads(action_input_match.group(1))
                    action_input["user_query"] = original_user_query
                    updated_txt = re.sub(r'<调用参数>.*?</调用参数>', 
                                         f'<调用参数>{json.dumps(action_input, ensure_ascii=False)}</调用参数>', 
                                         txt, flags=re.DOTALL)
                    history[-1]["content"] = updated_txt
            except Exception as e:
                print(f"处理调用参数出错: {str(e)}")  
        short_result = shorten_tool_result(result)
        history.append({
            "role":"user",
            "content": f"<工具调用结果>: {short_result}"
        })

        if done:
            print("===最终输出===")
            print(result)
            return result

        round_num+=1
        if round_num>max_round:
            print("超出上限")
            return

if __name__ == '__main__':
    # chat('你好')
    chat(
        '请分析192.168.0.110这台主机上周星期一和上上周星期一还有前天的cpu利用率，并作图给出分析报告')


# === config.py ===
# config.py
import json
import os
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("anomaly_detection")

WEIGHTS_SINGLE = {
    "Z-Score": 0.5,
    "CUSUM": 0.5
}

WEIGHTS_MULTI = {
    "ResidualComparison": 0.4, 
    "TrendDriftCUSUM": 0.25,   
    "ChangeRate": 0.15,
    "TrendSlope": 0.2
}

HIGH_ANOMALY_THRESHOLD = 0.7
MILD_ANOMALY_THRESHOLD = 0.4

DEFAULT_THRESHOLD_CONFIG = {
    "Z-Score": {
        "threshold": 3.5 
    },
    "CUSUM": {
        "drift_threshold": 6.0, 
        "k": 0.7  
    },
    "ResidualComparison": {
        "threshold": 3.5
    },
    "TrendDriftCUSUM": {
        "drift_threshold": 8.0 
    },
    "ChangeRate": {
        "threshold": 0.7
    },
    "TrendSlope": {
        "slope_threshold": 0.4, 
        "window": 5
    }
}

CONFIG_DIR = "config"
os.makedirs(CONFIG_DIR, exist_ok=True)
THRESHOLD_CONFIG_PATH = os.path.join(CONFIG_DIR, "threshold_config.json")

if not os.path.exists(THRESHOLD_CONFIG_PATH):
    try:
        with open(THRESHOLD_CONFIG_PATH, "w", encoding="utf-8") as f:
            json.dump(DEFAULT_THRESHOLD_CONFIG, f, ensure_ascii=False, indent=2)
        logger.info(f"已创建默认阈值配置文件: {THRESHOLD_CONFIG_PATH}")
    except Exception as e:
        logger.warning(f"无法创建默认配置文件: {e}")

try:
    with open(THRESHOLD_CONFIG_PATH, "r", encoding="utf-8") as f:
        USER_THRESHOLD_CONFIG = json.load(f)
        THRESHOLD_CONFIG = DEFAULT_THRESHOLD_CONFIG.copy()
        for method, config in USER_THRESHOLD_CONFIG.items():
            if method in THRESHOLD_CONFIG:
                THRESHOLD_CONFIG[method].update(config)
            else:
                THRESHOLD_CONFIG[method] = config
        logger.info(f"已加载用户阈值配置: {THRESHOLD_CONFIG_PATH}")
except Exception as e:
    logger.warning(f"无法读取阈值配置文件，使用默认值: {e}")
    THRESHOLD_CONFIG = DEFAULT_THRESHOLD_CONFIG



# === utils/ts_cache.py ===
# utils/ts_cache.py
import os
import pickle
import hashlib
import json
import requests
import datetime
from typing import List, Tuple, Optional, Dict, Any

CACHE_DIR = "cached_data"
os.makedirs(CACHE_DIR, exist_ok=True)

AIOPS_BACKEND_DOMAIN = 'https://aiopsbackend.cstcloud.cn'
AUTH = ('chelseyyycheng@outlook.com', 'UofV1uwHwhVp9tcTue')

def _cache_filename(ip: str, field: str, start_ts: int, end_ts: int) -> str:

    key = f"{ip}_{field}_{start_ts}_{end_ts}"
    h = hashlib.md5(key.encode('utf-8')).hexdigest()
    return os.path.join(CACHE_DIR, f"{h}.pkl")

def fetch_data_from_backend(ip: str, field: str, start_ts: int, end_ts: int) -> List[Tuple[int, float]]:
    #从后端API获取时序数据
    
    url = f"{AIOPS_BACKEND_DOMAIN}/api/v1/monitor/mail/metric/format-value/?start={start_ts}&end={end_ts}&instance={ip}&field={field}"
    resp = requests.get(url, auth=AUTH)
    
    if resp.status_code != 200:
        raise Exception(f"后端请求失败: {resp.status_code} => {resp.text}")
    
    j = resp.json()
    results = j.get("results", [])
    if not results:
        return []
    
    vals = results[0].get("values", [])
    arr = []
    
    def parse_ts(s):
        try:
            dt = datetime.datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
            return int(dt.timestamp())
        except:
            return 0
    
    for row in vals:
        if len(row) >= 2:
            tstr, vstr = row[0], row[1]
            t = parse_ts(tstr)
            try:
                v = float(vstr)
            except:
                v = 0.0
            arr.append([t, v])
    
    return arr

def ensure_cache_file(ip: str, field: str, start: str, end: str) -> str:
    #查询缓存文件，不存在从后端获取

    def to_int(s):
        dt = datetime.datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
        return int(dt.timestamp())
    
    st_i = to_int(start)
    et_i = to_int(end)
    
    fpath = _cache_filename(ip, field, st_i, et_i)
    
    if os.path.exists(fpath):
        print(f"[缓存] 从本地缓存读取 {ip} {field} 数据")
        return fpath
    
    try:
        print(f"[API] 从后端获取 {ip} {field} 数据")
        data = fetch_data_from_backend(ip, field, st_i, et_i)
        
        with open(fpath, "wb") as f:
            pickle.dump(data, f)
        
        print(f"[缓存] 数据已写入缓存 {fpath}")
        return fpath
    
    except Exception as e:
        print(f"[错误] 获取数据失败: {e}")
        return str(e)

def load_series_from_cache(ip: str, field: str, start: str, end: str) -> List[Tuple[int, float]]:

    
    cache_file = ensure_cache_file(ip, field, start, end)
    
    if not os.path.exists(cache_file):
        raise Exception(f"缓存文件不存在: {cache_file}")
    
    with open(cache_file, "rb") as f:
        data = pickle.load(f)
    
    return data

# === utils/__init__.py ===
# utils/__init__.py
"""
工具函数模块，包含各种通用辅助函数
"""
from utils.time_utils import group_anomaly_times
from utils.ts_cache import ensure_cache_file, load_series_from_cache

# === utils/time_utils.py ===
# utils/time_utils.py
import re
import datetime
import dateparser

def parse_time_expressions(raw_text: str):
    #解析自然语言时间表达式
    
    segments = re.split(r'[,\uFF0C\u3001\u0026\u002C\u002F\u0020\u0026\u2014\u2013\u2014\u006E\u005E]|和|与|及|还有|、', raw_text)
    results = []
    
    for seg in segments:
        seg = seg.strip()
        if not seg:
            continue

        dt = dateparser.parse(seg, languages=['zh', 'en'], settings={"PREFER_DATES_FROM": "past"})
        if dt is None:
            results.append({"start": 0, "end": 0, "error": f"无法解析: {seg}"})
        else:
            day_s = datetime.datetime(dt.year, dt.month, dt.day, 0, 0, 0)
            day_e = datetime.datetime(dt.year, dt.month, dt.day, 23, 59, 59)
            
            start_str = day_s.strftime("%Y-%m-%d %H:%M:%S")
            end_str = day_e.strftime("%Y-%m-%d %H:%M:%S")
            
            results.append({
                "start": int(day_s.timestamp()),
                "end": int(day_e.timestamp()),
                "error": "",
                "start_str": start_str,
                "end_str": end_str
            })
    
    return results

def group_anomaly_times(anomalies, max_gap=1800):
    #将时间戳列表分组为连续的时间区间
    if not anomalies:
        return []
    
    sorted_anomalies = sorted(anomalies)
    
    intervals = []
    cur_start = sorted_anomalies[0]
    cur_end = sorted_anomalies[0]
    
    for t in sorted_anomalies[1:]:
        if t - cur_end <= max_gap:
            cur_end = t
        else:
            intervals.append((cur_start, cur_end))
            cur_start = t
            cur_end = t
    
    intervals.append((cur_start, cur_end))
    
    return intervals

def format_timestamp(ts):
    try:
        return datetime.datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M:%S")
    except:
        return str(ts)

def to_timestamp(time_str):
    try:
        dt = datetime.datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
        return int(dt.timestamp())
    except Exception as e:
        raise ValueError(f"时间字符串格式错误: {e}")

# === output/report_generator.py ===
# output/report_generator.py
import os
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Union

import config
from output.visualization import generate_summary_echarts_html

def convert_markdown_to_html(markdown_text):
    html = markdown_text
    html = re.sub(r'^## (.*?)$', r'<h2>\1</h2>', html, flags=re.MULTILINE)
    html = re.sub(r'^### (.*?)$', r'<h3>\1</h3>', html, flags=re.MULTILINE)
    html = re.sub(r'^#### (.*?)$', r'<h4>\1</h4>', html, flags=re.MULTILINE)
    html = re.sub(r'^# (.*?)$', r'<h1>\1</h1>', html, flags=re.MULTILINE)
    html = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', html)
    html = re.sub(r'\*(.*?)\*', r'<em>\1</em>', html)

    html = re.sub(r'^- (.*?)$', r'<li>\1</li>', html, flags=re.MULTILINE)

    li_pattern = r'(<li>.*?</li>)+'
    
    def replace_list(match):
        list_items = match.group(0)
        return f'<ul>{list_items}</ul>'
    
    html = re.sub(li_pattern, replace_list, html, flags=re.DOTALL)
    html = re.sub(r'^(\d+)\. (.*?)$', r'<li>\2</li>', html, flags=re.MULTILINE)

    paragraphs = []
    buffer = ""
    
    for line in html.split('\n'):
        line = line.strip()
        if line and not line.startswith('<') and not line.endswith('>'):
            if buffer:
                buffer += " " + line
            else:
                buffer = line
        else:
            if buffer:
                paragraphs.append(f'<p>{buffer}</p>')
                buffer = ""
            if line:
                paragraphs.append(line)
    
    if buffer:
        paragraphs.append(f'<p>{buffer}</p>')
    
    html = '\n'.join(paragraphs)
    
    return html

def generate_llm_report(result, report_type="single", user_query=""):
    #大模型生成异常检测分析报告

    classification = result.get('classification', '未知')
    score = result.get('composite_score', 0)
    
    anomaly_count = len(result.get('anomaly_times', []))
    interval_count = len(result.get('anomaly_intervals', []))
    
    method_results = result.get('method_results', [])
    method_details = []
    
    for method in method_results:
        if isinstance(method, dict):
            method_info = method
        else:
            method_info = method.to_dict() if hasattr(method, 'to_dict') else vars(method)
        
        method_name = method_info.get('method', '未知方法')
        anomalies = method_info.get('anomalies', [])
        intervals = method_info.get('intervals', [])
        explanations = method_info.get('explanation', [])
        
        if anomalies or intervals or explanations:
            detail = {
                "method": method_name,
                "anomaly_count": len(anomalies),
                "interval_count": len(intervals),
                "explanations": explanations[:3]  #限制长度
            }
            method_details.append(detail)
    
    if report_type == "single":
        prompt = f"""
        请为以下单序列异常检测结果生成一份详细的分析报告，报告应包含以下方面：
        1. 总体异常状况摘要
        2. 各检测方法发现的问题分析
        3. 可能的原因分析
        4. 建议的后续操作

        检测结果信息：
        用户查询: {user_query}
        总体分类: {classification}
        综合得分: {score:.2f}
        发现异常点数: {anomaly_count}

        检测方法详情:
        {json.dumps(method_details, ensure_ascii=False, indent=2)}

        请注意以下要求：
        生成的报告应该使用Markdown格式，包含标题(##)、子标题(###)、列表项等
        每个部分应该使用二级标题(##)，内容要精炼但信息量丰富
        不要简单重复上述数据，而是提供有价值的见解和分析
        总长度控制在600字左右
"""
    else:  #multi
        prompt = f"""
        请为以下多序列对比异常检测结果生成一份详细的分析报告，报告应包含以下方面：
        1. 两个序列的对比摘要
        2. 差异模式与趋势分析
        3. 各检测方法发现的问题详解
        4. 异常的可能原因
        5. 建议的后续操作

        检测结果信息：
        用户查询: {user_query}
        总体分类: {classification}
        综合得分: {score:.2f}
        发现异常点数: {anomaly_count}
        发现异常区间数: {interval_count}

        检测方法详情:
        {json.dumps(method_details, ensure_ascii=False, indent=2)}

        请注意以下要求：
        生成的报告应该使用Markdown格式，包含标题(##)、子标题(###)、列表项等
        每个部分应该使用二级标题(##)，内容要精炼但信息量丰富
        不要简单重复上述数据，而是提供有价值的见解和分析
        总长度控制在600字左右
"""

    # 调用大模型生成报告
    messages = [
        {"role": "system", "content": "你是一位专业的系统运维分析师，擅长解读时序数据异常检测结果并提供深入分析。你的回复应使用规范的Markdown格式。"},
        {"role": "user", "content": prompt}
    ]
    
    try:
        import sys, os
        sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
        from agent import llm_call
        
        response = llm_call(messages)
        if response and 'content' in response:
            return response['content']
        else:
            print("LLM返回格式不正确或为空")
            return f"## 异常检测报告\n\n**分类**: {classification}\n**得分**: {score:.2f}\n**异常点数**: {anomaly_count}"
    except Exception as e:
        print(f"LLM调用失败: {e}")
        return f"## 异常检测报告\n\n**分类**: {classification}\n**得分**: {score:.2f}\n**异常点数**: {anomaly_count}"


def generate_common_report(series_data, metadata, detection_type, user_query=""):
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if detection_type == "single":
        # 单序列
        series = series_data
        ip = metadata.get("ip", "unknown")
        field = metadata.get("field", "unknown")
        
        base_dir = f"output/plots/{ip}_{field}_{timestamp}"
        os.makedirs(base_dir, exist_ok=True)
        
        chart_title = f"{ip} {field} 异常检测汇总图"
        
        from analysis.single_series import analyze_single_series
        result = analyze_single_series(series)
        series2 = None
        
        start_time = datetime.fromtimestamp(series[0][0]).strftime("%Y-%m-%d %H:%M:%S") if series else "未知"
        end_time = datetime.fromtimestamp(series[-1][0]).strftime("%Y-%m-%d %H:%M:%S") if series else "未知"
        
        series_info = {"ip": ip, "field": field, "start": start_time, "end": end_time}
        
    else:  # multi
        series1, series2 = series_data
        ip1 = metadata.get("ip1", "unknown")
        ip2 = metadata.get("ip2", "unknown")
        field = metadata.get("field", "unknown")
        
        base_dir = f"output/plots/{ip1}_{ip2}_{field}_{timestamp}"
        os.makedirs(base_dir, exist_ok=True)
        
        chart_title = f"{ip1} vs {ip2} {field} 对比异常检测汇总图"
        
        from analysis.multi_series import analyze_multi_series
        result = analyze_multi_series(series1, series2)
        
        series_info = {"ip1": ip1, "ip2": ip2, "field": field}
    
    method_results = result["method_results"]
    summary_path = os.path.join(base_dir, "summary.html")
    chart_path, tooltip_map = generate_summary_echarts_html(
        series_data if detection_type == "single" else series_data[0], 
        series2, 
        method_results, 
        summary_path, 
        title=chart_title
    )
    
    print(f"调用大模型生成报告，用户查询: {user_query}")
    llm_analysis = generate_llm_report(result, detection_type, user_query)
    print(f"大模型生成的报告长度: {len(llm_analysis)}")
    
    final_report_path = os.path.join(base_dir, "final_report.html")
    generate_report_html(
        user_query, chart_path, method_results, tooltip_map, final_report_path, 
        composite_score=result["composite_score"],
        classification=result["classification"],
        llm_analysis=llm_analysis,
        is_multi_series=(detection_type == "multi")
    )
    
    result_dict = {
        "classification": result["classification"],
        "composite_score": result["composite_score"],
        "anomaly_times": result["anomaly_times"],
        "method_results": [mr.to_dict() if hasattr(mr, 'to_dict') else mr for mr in method_results],
        "report_path": final_report_path,
        "llm_analysis": llm_analysis
    }
    
    if detection_type == "multi":
        result_dict["anomaly_intervals"] = result["anomaly_intervals"]
    
    return result_dict


def generate_report_single(series, ip, field, user_query):
    metadata = {"ip": ip, "field": field}
    return generate_common_report(series, metadata, "single", user_query)


def generate_report_multi(series1, series2, ip1, ip2, field, user_query):
    metadata = {"ip1": ip1, "ip2": ip2, "field": field}
    return generate_common_report((series1, series2), metadata, "multi", user_query)


def generate_report_html(user_query, chart_path, detection_results, tooltip_map, output_path, 
                          composite_score=0.0, classification="正常", llm_analysis=None, is_multi_series=False):
    
    #异常检测HTML报告
    
    if classification == "高置信度异常":
        alert_class = "alert-danger"
    elif classification == "轻度异常":
        alert_class = "alert-warning"
    else:
        alert_class = "alert-success"

    methods_summary_html = ""
    for result in detection_results:
        if isinstance(result, dict):
            method = result.get("method", "未知")
            description = result.get("description", "")
        else:
            method = result.method if hasattr(result, 'method') else "未知"
            description = result.description if hasattr(result, 'description') else ""
            
        methods_summary_html += f"""
        <div class="card mb-2">
            <div class="card-header">
                <strong>{method}</strong>
            </div>
            <div class="card-body">
                <p>{description}</p>
            </div>
        </div>
        """
   
    formatted_llm_analysis = ""
    if llm_analysis:
        formatted_llm_analysis = convert_markdown_to_html(llm_analysis)
    
    report_title = "多序列对比异常检测报告" if is_multi_series else "时序数据异常检测报告"
    
    html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>{report_title}</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {{ padding: 20px; }}
        .iframe-container {{ width: 100%; height: 650px; border: none; }}
        .markdown-content {{ line-height: 1.6; }}
        .markdown-content h3 {{ margin-top: 20px; margin-bottom: 15px; color: #333; font-size: 1.4rem; }}
        .markdown-content h2 {{ margin-top: 25px; margin-bottom: 15px; color: #222; font-size: 1.6rem; }}
        .markdown-content h1 {{ margin-top: 30px; margin-bottom: 20px; color: #111; font-size: 1.8rem; }}
        .markdown-content p {{ margin-bottom: 15px; font-size: 1rem; }}
        .markdown-content strong {{ font-weight: 600; color: #444; }}
        .markdown-content ul, .markdown-content ol {{ margin-bottom: 15px; padding-left: 25px; }}
        .markdown-content li {{ margin-bottom: 5px; }}
        .info-box {{ 
            border: 1px solid #ddd; 
            border-radius: 5px; 
            padding: 10px; 
            margin-bottom: 15px; 
            background-color: #f9f9f9; 
        }}
        .hint-text {{
            font-style: italic;
            color: #666;
            margin-top: 5px;
            font-size: 0.9em;
        }}
        .ai-analysis {{
            background-color: #f8f9fa;
            border-left: 4px solid #17a2b8;
            padding: 15px;
            margin: 20px 0;
        }}
        .ai-analysis-title {{
            color: #17a2b8;
            font-weight: 600;
            margin-bottom: 10px;
            font-size: 1.2rem;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1 class="mt-4 mb-4">{report_title}</h1>
        
        <div class="card mb-4">
            <div class="card-header">
                <strong>用户问题</strong>
            </div>
            <div class="card-body">
                <p>{user_query}</p>
            </div>
        </div>
        
        <div class="card mb-4">
            <div class="card-header">
                <strong>分析结论</strong>
            </div>
            <div class="card-body">
                <div class="alert {alert_class}">
                    <h4>综合判定: {classification}</h4>
                    <p>综合得分: {composite_score:.2f}</p>
                </div>
                
                <!-- 大模型分析报告 -->
                <div class="ai-analysis">
                    <div class="markdown-content">
                        {formatted_llm_analysis if llm_analysis else "未生成分析报告"}
                    </div>
                </div>
            </div>
        </div>
        
        <div class="card mb-4">
            <div class="card-header">
                <strong>异常检测图表</strong>
            </div>
            <div class="card-body p-0">
                <div class="info-box">
                    <p class="hint-text">提示: 将鼠标悬停在异常点上可查看详细信息。</p>
                </div>
                <iframe class="iframe-container" src="{os.path.basename(chart_path)}"></iframe>
            </div>
        </div>
        
        <h2 class="mt-4 mb-3">检测方法摘要</h2>
        {methods_summary_html}
        
        <footer class="mt-5 text-center text-muted">
            <p>生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </footer>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>"""

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(html)
    return output_path

def get_anomaly_detection_report(result, detection_type):

    llm_analysis = generate_llm_report(result, detection_type, result.get("user_query", ""))

    if detection_type == "single":
        title = "单序列异常检测报告"
        anomaly_info = f"**异常点数**：{len(result['anomaly_times'])}"
    else:  # multi
        title = "多序列对比异常检测报告"
        anomaly_info = f"**异常段数**：{len(result.get('anomaly_intervals', []))}"
    
    final_report = f"""
    ## {title}
    **结论**：{result['classification']}  
    **综合得分**：{result['composite_score']:.2f}  
{anomaly_info}

{llm_analysis}
**图表路径**：{result['report_path']}
"""
    return final_report

# === output/visualization.py ===
# output/visualization.py
import json
import uuid
import os
import time
from datetime import datetime
from typing import List, Dict, Tuple, Any, Optional, Union
from utils.time_utils import format_timestamp

def process_anomaly_points(detection_results, series_data, timestamps):
    
    mark_points = []
    tooltip_map = {}
    point_counter = 1
    
    for result in detection_results:
        if not hasattr(result, 'visual_type') or result.visual_type != "point":
            continue
            
        anomalies = result.anomalies if hasattr(result, 'anomalies') else []
        explanations = result.explanation if hasattr(result, 'explanation') else []
        
        for i, ts in enumerate(anomalies):
            value = next((v for t, v in series_data if t == ts), None)
            if value is None:
                continue

            explanation = ""
            if i < len(explanations):
                explanation = explanations[i]
            
            tooltip_map[point_counter] = {
                "method": result.method if hasattr(result, 'method') else "未知方法",
                "ts": ts,
                "value": value,
                "explanation": explanation
            }
            
            mark_points.append({
                "coord": [ts * 1000, value],
                "symbol": "circle",
                "symbolSize": 8,
                "itemStyle": {"color": "red"},
                "label": {"formatter": f"#{point_counter}", "show": True, "position": "top"},
                "anomalyInfo": {
                    "id": point_counter,
                    "method": result.method if hasattr(result, 'method') else "未知方法",
                    "timestamp": format_timestamp(ts),
                    "value": value,
                    "explanation": explanation
                }
            })
            
            point_counter += 1
            
    return mark_points, tooltip_map

def process_anomaly_ranges(detection_results, timestamps):
    
    mark_areas = []
    tooltip_map = {}
    point_counter = 1
    
    for result in detection_results:
        if not hasattr(result, 'visual_type') or result.visual_type not in ("range", "curve"):
            continue
            
        intervals = result.intervals if hasattr(result, 'intervals') else []
        explanations = result.explanation if hasattr(result, 'explanation') else []
        
        for i, (start, end) in enumerate(intervals):
            area_explanation = ""
            if i < len(explanations):
                area_explanation = explanations[i]
                
            tooltip_map[point_counter] = {
                "method": result.method if hasattr(result, 'method') else "未知方法",
                "ts_start": start,
                "ts_end": end,
                "explanation": area_explanation
            }
            
            mark_areas.append({
                "itemStyle": {"color": "rgba(255, 100, 100, 0.2)"},
                "label": {"show": True, "position": "top", "formatter": f"#{point_counter}"},
                "xAxis": start * 1000,
                "xAxis2": end * 1000,
                "anomalyInfo": {
                    "id": point_counter,
                    "method": result.method if hasattr(result, 'method') else "未知方法",
                    "startTime": format_timestamp(start),
                    "endTime": format_timestamp(end),
                    "explanation": area_explanation
                }
            })
            
            point_counter += 1
            
    return mark_areas, tooltip_map

def process_auxiliary_curves(detection_results, timestamps):
    #处理辅助曲线数据
    
    extra_series = []
    for result in detection_results:
        # 跳过非曲线类型或没有辅助曲线的结果
        if not hasattr(result, 'visual_type') or result.visual_type != "curve":
            continue
        if not hasattr(result, 'auxiliary_curve') or not result.auxiliary_curve:
            continue
    
        #是否使用第二个Y轴
        use_second_yaxis = False
        if hasattr(result, 'method'):
            use_second_yaxis = result.method in ["CUSUM", "TrendDriftCUSUM"]
            
        yAxisIndex = 1 if use_second_yaxis else 0
        curve_data = [[t * 1000, v] for t, v in result.auxiliary_curve]
        
        extra_series.append({
            "name": f"{result.method if hasattr(result, 'method') else '辅助'} 辅助曲线",
            "type": "line",
            "yAxisIndex": yAxisIndex,
            "data": curve_data,
            "lineStyle": {"type": "dashed", "width": 1.5},
            "itemStyle": {"color": "#EE6666"},
            "showSymbol": False
        })
            
    return extra_series

def prepare_series_data(series1, series2=None):
   
    series_list = [{
        "name": series2 is not None and "上周CPU利用率" or "原始序列",
        "type": "line",
        "data": [[t * 1000, v] for t, v in series1],
        "symbolSize": 0,  # 减小正常点的大小
        "lineStyle": {"width": 2},
        "itemStyle": {"color": "#5470C6"}
    }]

    if series2 is not None:
        #时间范围对齐
        min_time = min(series1[0][0], series2[0][0])
        offset1 = series1[0][0] - min_time 
        offset2 = series2[0][0] - min_time
        
        adjusted_series2 = [(t - offset2 + offset1, v) for t, v in series2]
        
        series_list.append({
            "name": "这周CPU利用率",
            "type": "line",
            "data": [[t * 1000, v] for t, v in adjusted_series2],
            "symbolSize": 0,
            "lineStyle": {"width": 2},
            "itemStyle": {"color": "#91CC75"}
        })
        
        # 添加差值曲线
        if len(series1) == len(adjusted_series2):
            diff_data = []
            for i in range(len(series1)):
                diff_value = round(series1[i][1] - adjusted_series2[i][1], 3)
                diff_data.append([series1[i][0] * 1000, diff_value])
                
            series_list.append({
                "name": "差值曲线",
                "type": "line",
                "data": diff_data,
                "symbolSize": 0, 
                "lineStyle": {"width": 1, "type": "dashed"},
                "itemStyle": {"color": "#EE6666"}
            })
    
    return series_list

def generate_summary_echarts_html(series1, series2=None, detection_results=None, output_path=None, title="时序异常检测汇总图"):
    
    if detection_results is None:
        detection_results = []
        
    chart_id = f"chart_summary_{uuid.uuid4().hex[:8]}"
    timestamps = [t for t, _ in series1]
    
    series_list = prepare_series_data(series1, series2)
    
    mark_points, tooltip_map_points = process_anomaly_points(detection_results, series1, timestamps)
    mark_areas, tooltip_map_areas = process_anomaly_ranges(detection_results, timestamps)
    extra_series = process_auxiliary_curves(detection_results, timestamps)
    
    series_list.extend(extra_series)
    
    tooltip_map = {**tooltip_map_points, **tooltip_map_areas}
    
    need_second_yaxis = any(s.get("yAxisIndex", 0) == 1 for s in series_list)

    option = {
        "title": {"text": title, "left": "center"},
        "tooltip": {
            "trigger": "axis", 
            "axisPointer": {"type": "cross"}, 
            "confine": True,
            "backgroundColor": "rgba(255, 255, 255, 0.9)",
            "borderColor": "#ccc",
            "borderWidth": 1,
            "textStyle": {
                "color": "#333"
            },
            "extraCssText": "box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);"
        },
        "legend": {"top": 30, "data": [s["name"] for s in series_list]},
        "grid": {"left": "3%", "right": need_second_yaxis and "8%" or "4%", "bottom": "8%", "containLabel": True},
        "toolbox": {
            "feature": {
                "saveAsImage": {},
                "dataZoom": {},
                "restore": {}
            }
        },
        "xAxis": {
            "type": "time",
            "name": "时间",
            "axisLabel": {
                "formatter": "{yyyy}-{MM}-{dd} {HH}:{mm}",
                "rotate": 30,
                "margin": 15
            }
        },
        "yAxis": [
            {"type": "value", "name": "数值", "position": "left"}
        ],
        "series": series_list,
        "dataZoom": [
            {"type": "slider", "show": True, "xAxisIndex": [0], "start": 0, "end": 100, "bottom": 10},
            {"type": "inside", "xAxisIndex": [0], "start": 0, "end": 100}
        ]
    }
    
    if need_second_yaxis:
        option["yAxis"].append({
            "type": "value",
            "name": "辅助曲线值",
            "position": "right",
            "splitLine": {"show": False}
        })

    if mark_points:
        series_list[0]["markPoint"] = {
            "data": mark_points, 
            "symbolSize": 8,
            "emphasis": {"scale": True},
            "label": {"show": True}
        }
    
    if mark_areas:
        series_list[0]["markArea"] = {
            "data": [[{"xAxis": area["xAxis"], "itemStyle": area["itemStyle"], "anomalyInfo": area["anomalyInfo"]}, 
                     {"xAxis": area["xAxis2"]}] for area in mark_areas],
            "emphasis": {"focus": "self"}
        }

    #异常点的tooltip
    html = generate_html_template(chart_id, option, title)

    if output_path:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html)
        return output_path, tooltip_map
    else:
        return html, tooltip_map

def generate_html_template(chart_id, option, title):
    return f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>{title}</title>
    <script src="https://cdn.jsdelivr.net/npm/echarts@5/dist/echarts.min.js"></script>
    <style>
        body {{ margin: 0; padding: 0; }}
        #container {{ width: 100%; height: 100vh; }}
        #{chart_id} {{ width: 100%; height: 650px; }}
        
        /* 异常点提示样式 */
        .anomaly-tooltip {{
            background-color: rgba(255, 255, 255, 0.9);
            border: 1px solid #ccc;
            border-radius: 4px;
            padding: 10px;
            box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);
            font-family: Arial, sans-serif;
            font-size: 14px;
            color: #333;
            max-width: 300px;
        }}
        .anomaly-tooltip-title {{
            font-weight: bold;
            margin-bottom: 5px;
            color: #d23;
        }}
        .anomaly-tooltip-item {{
            display: flex;
            margin: 2px 0;
        }}
        .anomaly-tooltip-label {{
            font-weight: bold;
            margin-right: 8px;
            color: #666;
            min-width: 60px;
        }}
        .anomaly-tooltip-value {{
            color: #333;
        }}
    </style>
</head>
<body>
    <div id="container">
        <div id="{chart_id}"></div>
    </div>
    <script>
        var chart = echarts.init(document.getElementById('{chart_id}'));
        var option = {json.dumps(option, ensure_ascii=False)};
        
        chart.on('mouseover', function(params) {{
            if (params.componentType === 'markPoint' && params.data && params.data.anomalyInfo) {{
                var info = params.data.anomalyInfo;
                var content = 
                    '<div class="anomaly-tooltip">' +
                    '<div class="anomaly-tooltip-title">异常点 #' + info.id + '</div>' +
                    '<div class="anomaly-tooltip-item"><span class="anomaly-tooltip-label">方法:</span><span class="anomaly-tooltip-value">' + info.method + '</span></div>' + 
                    '<div class="anomaly-tooltip-item"><span class="anomaly-tooltip-label">时间:</span><span class="anomaly-tooltip-value">' + info.timestamp + '</span></div>' + 
                    '<div class="anomaly-tooltip-item"><span class="anomaly-tooltip-label">数值:</span><span class="anomaly-tooltip-value">' + info.value.toFixed(2) + '</span></div>' + 
                    '<div class="anomaly-tooltip-item"><span class="anomaly-tooltip-label">说明:</span><span class="anomaly-tooltip-value">' + info.explanation + '</span></div>' +
                    '</div>';
                
                chart.setOption({{
                    tooltip: {{
                        formatter: content,
                        extraCssText: 'padding: 0; border: none; background: transparent;'
                    }}
                }});
            }}
            else if (params.componentType === 'markArea' && params.data && params.data[0] && params.data[0].anomalyInfo) {{
                var info = params.data[0].anomalyInfo;
                var content = 
                    '<div class="anomaly-tooltip">' +
                    '<div class="anomaly-tooltip-title">异常区间 #' + info.id + '</div>' +
                    '<div class="anomaly-tooltip-item"><span class="anomaly-tooltip-label">方法:</span><span class="anomaly-tooltip-value">' + info.method + '</span></div>' + 
                    '<div class="anomaly-tooltip-item"><span class="anomaly-tooltip-label">开始:</span><span class="anomaly-tooltip-value">' + info.startTime + '</span></div>' + 
                    '<div class="anomaly-tooltip-item"><span class="anomaly-tooltip-label">结束:</span><span class="anomaly-tooltip-value">' + info.endTime + '</span></div>' + 
                    '<div class="anomaly-tooltip-item"><span class="anomaly-tooltip-label">说明:</span><span class="anomaly-tooltip-value">' + info.explanation + '</span></div>' +
                    '</div>';
                
                chart.setOption({{
                    tooltip: {{
                        formatter: content,
                        extraCssText: 'padding: 0; border: none; background: transparent;'
                    }}
                }});
            }}
        }});
        
        chart.on('mouseout', function(params) {{
            if (params.componentType === 'markPoint' || params.componentType === 'markArea') {{
                //恢复默认tooltip
                chart.setOption({{
                    tooltip: {{
                        formatter: null,
                        extraCssText: option.tooltip.extraCssText
                    }}
                }});
            }}
        }});
        
        chart.setOption(option);
        window.addEventListener('resize', function() {{
            chart.resize();
        }});
    </script>
</body>
</html>"""

# === output/__init__.py ===


# === test/testfile.py ===
import os

def dump_project_code(project_root, output_file="project_code_dump.txt"):
    with open(output_file, "w", encoding="utf-8") as out:
        for root, _, files in os.walk(project_root):
            if "__pycache__" in root:
                continue
            for file in files:
                if file.endswith(".py"):
                    file_path = os.path.join(root, file)
                    rel_path = os.path.relpath(file_path, project_root)
                    out.write(f"# === {rel_path} ===\n")
                    with open(file_path, "r", encoding="utf-8") as f:
                        out.write(f.read())
                    out.write("\n\n")

project_path = "/home/cnic/aiagent1"  
dump_project_code(project_path)


# === test/mail_test.py ===
#! /usr/bin/env python3
import re
import json
import datetime
import requests
from django.conf import settings

AIOPS_BACKEND_DOMAIN = 'https://aiopsbackend.cstcloud.cn'
LLM_URL = 'http://10.16.1.16:58000/v1/chat/completions'

AUTH = ('chelseyyycheng@outlook.com', 'UofV1uwHwhVp9tcTue')

tools = [
    {
        "type": "function",
        "function": {
            "name": "请求智能运管后端Api，获取指标项的时序数据",
            "description": "请求智能运管后端Api，获取指标项的时序数据",
            "parameters": {
                "type": "object",
                "properties": {
                    "ip": {
                        "type": "string",
                        "description": "要查询的ip"
                    },
                    "start": {
                        "type": "string",
                        "description": "日期，格式为 Y-%m-%d %H:%M:%S"
                    },
                    "end": {
                        "type": "string",
                        "description": "日期，格式为 Y-%m-%d %H:%M:%S"
                    },
                    "field": {
                        "type": "string",
                        "description": "监控项名称，只可在监控项列表中选择"
                    },
                },
                "required": ["ip", "start", "end", "field"],
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "请求智能运管后端Api，查询监控实例有哪些监控项",
            "description": "请求智能运管后端Api，查询监控实例有哪些监控项",
            "parameters": {
                "type": "object",
                "properties": {
                    "service": {
                        "type": "string",
                        "description": "要查询的系统名称"
                    },
                    "instance": {
                        "type": "string",
                        "description": "要查询的监控实例"
                    },
                },
                "required": ["service", "instance"],

            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "请求智能运管后端Api，查询监控实例之间的拓扑关联关系",
            "description": "监控实例上联了哪些监控实例列表，下联了哪些监控实例列表",
            "parameters": {
                "type": "object",
                "properties": {
                    "service": {
                        "type": "string",
                        "description": "要查询的系统名称"
                    },
                    "instance_ip": {
                        "type": "string",
                        "description": "要查询的监控实例的IP地址"
                    },
                },
                "required": ["service", 'instance_ip'],
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "请求智能运管后端Api，查询监控服务的资产情况和监控实例",
            "description": "请求智能运管后端Api，查询监控服务的资产情况和监控实例",
            "parameters": {
                "type": "object",
                "properties": {
                    "service": {
                        "type": "string",
                        "description": "要查询的系统名称"
                    },
                },
                "required": ["service"],

            }
        }
    },

]


def monitor_item_list(ip):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/monitor/mail/machine/field/?instance={ip}'
    resp = requests.get(url=url, auth=AUTH)
    text = json.loads(resp.text)
    result = dict()
    if resp.status_code == 200:
        for item in text:
            result[item.get('field')] = item.get('purpose')
        return result
    else:
        return text


def get_service_asset(service):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/property/mail/?ordering=num_id&page=1&page_size=2000'
    resp = requests.get(url=url, auth=AUTH)
    text = json.loads(resp.text)
    results = text.get('results')
    item_list = []
    for _ in results:
        _['category'] = _.get('category').get('name')
        _["ip_set"] = [_.get("ip") for _ in _.get('ip_set')]
        _.pop('num_id')
        _.pop('creation')
        _.pop('modification')
        _.pop('remark')
        _.pop('sort_weight')
        _.pop('monitor_status')
        for k, v in _.copy().items():
            if not v or v == '无':
                _.pop(k)
        item_list.append(_)
    return item_list


def get_service_asset_edges(service, instance_ip):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/property/mail/topology/search?instance={instance_ip}'
    resp = requests.get(url=url, auth=AUTH)
    text = json.loads(resp.text)
    # print(text)
    return text


def get_monitor_metric_value(ip, start, end, field):
    metric_field_list = monitor_item_list(ip)
    if field not in metric_field_list.keys():
        return f"未知的监控项：{field}"
    # 查询监控指标情况
    start_timestamp = datetime.datetime.strptime(start, "%Y-%m-%d %H:%M:%S").timestamp()
    end_timestamp = datetime.datetime.strptime(end, "%Y-%m-%d %H:%M:%S").timestamp()
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/monitor/mail/metric/format-value/?start={start_timestamp}&end={end_timestamp}&instance={ip}&field={field}'
    resp = requests.get(url=url, auth=AUTH)
    text = resp.text
    text = json.loads(text)
    return text


# ------------------------------------------------------------------------


def llm_call(messages):
    # for _ in messages:
    #     print(_)
    #     print('\n')
    data = {
        "model": "Qwen2.5-14B-Instruct",
        "temperature": 0.1,
        "messages": messages,
    }
    response = requests.post(LLM_URL, json=data)
    if response.status_code == 200:
        response_data = response.json()
        if 'choices' in response_data and len(response_data['choices']) > 0:
            generated_content = response_data['choices'][0]['message']['content']
            # print('##Token使用情况##:\n\n', response_data['usage'])
            # print('------------------\n\n')
            return response_data['choices'][0]['message']
        else:
            print(response_data)
            raise Exception("模型没有返回信息")
    else:
        print(f'Error: {response.status_code}')
        print(response.text)


def init_message_by_role(role, content):
    message = {
        'role': role,
        "content": content
    }
    return message


def parse_llm_response(llm_resp_content):
    invalid_value = ["空", '无']
    thought_match = re.search(r'<思考过程>(.*)</思考过程>', llm_resp_content, re.S)
    if thought_match and thought_match[0] not in invalid_value:
        thought = thought_match.group(1)
    else:
        thought = ""
    action_match = re.search(r'<工具调用>(.*)</工具调用>', llm_resp_content)
    if action_match and action_match[0] not in invalid_value:
        action = action_match.group(1)
    else:
        action = ""
    action_input_match = re.search(r'<调用参数>(.*)</调用参数>', llm_resp_content)
    if action_input_match and action_input_match[0] not in invalid_value:
        action_input = action_input_match.group(1)
    else:
        action_input = ""
    if "<最终答案>" in llm_resp_content and "</最终答案>" not in llm_resp_content:
        llm_resp_content += "</最终答案>"
    final_answer_match = re.search(r'<最终答案>(.*)</最终答案>', llm_resp_content, re.S)
    if final_answer_match and final_answer_match[0] not in invalid_value:
        final_answer = final_answer_match.group(1)
    else:
        final_answer = ""
    result = {
        'thought': thought,
        'action': action,
        'action_input': action_input,
        'final_answer': final_answer,
    }
    return result


def react(llm_resp_content):
    is_final = False
    llm_parsed_dict = parse_llm_response(llm_resp_content)
    # print("##解析后的参数", llm_parsed_dict)
    action = llm_parsed_dict.get('action')
    action_input = llm_parsed_dict.get('action_input')
    final_answer = llm_parsed_dict.get('final_answer')
    # print("当前的大模型解析", llm_parsed_dict)
    if action and action_input:
        # print("##调用函数##", action, action_input)
        action_input = json.loads(action_input)
        if action == '请求智能运管后端Api，获取指标项的时序数据':
            return get_monitor_metric_value(**action_input), is_final
        if action == '请求智能运管后端Api，查询监控实例有哪些监控项':
            return monitor_item_list(action_input.get('instance')), is_final
        if action == '请求智能运管后端Api，查询监控服务的资产情况和监控实例':
            return get_service_asset(action_input.get('service')), is_final
        if action == "请求智能运管后端Api，查询监控实例之间的拓扑关联关系":
            return get_service_asset_edges(**action_input), is_final
    if final_answer:
        is_final = True
        return final_answer, is_final
    else:
        result = """
生成的文本格式有误，严格按照以下指定格式生成响应：
```
<思考过程>你的思考过程</思考过程>
<工具调用>工具名称（必须是{tool_names}之一），如果不调用工具，则为空</工具调用>
<调用参数>工具输入参数（严格符合工具描述格式）</调用参数>
<最终答案>用户问题的最终结果（知道问题的最终答案时返回）</最终答案>
```
"""
        return result, is_final


def stream_response_format(category, content):
    data = f'data: {json.dumps({category: content}, ensure_ascii=False)}\n\n'
    return data


def chat(user_content):
    system_template = '''你是一个严格遵守格式规范的用于运维功能，运维数据可视化，运行于生产环境的ReAct智能体，你叫小助手，必须按以下格式处理请求：

    可用工具：
    {tools}

    处理规则：
    1.根据用户的问题来自行判断是否要调用工具以及调用哪个工具
    2.每次只能调用一个工具
    3.不能伪造数据
    3.严格按照以下xml格式生成响应文本：
    ```
    <思考过程>你的思考过程</思考过程>
    <工具调用>工具名称（必须是{tool_names}之一），如果不调用工具，则为空</工具调用>
    <调用参数>工具输入参数（严格符合工具描述格式）</调用参数>
    <最终答案>用户问题的最终结果（知道问题的最终答案时返回）</最终答案>
    ```
    '''
    history = list()
    history.append(init_message_by_role(
        role='system',
        content=system_template.format(
            tools=json.dumps(tools, ensure_ascii=False),
            tool_names=json.dumps([tool["function"]["name"] for tool in tools], ensure_ascii=False), )
    ))
    current_datetime = str(datetime.datetime.now()).split('.')[0]
    history.append(init_message_by_role(role='user', content=f'当前时间是 {current_datetime}'))
    history.append(init_message_by_role(role='user', content=user_content))
    count = 1
    while True:
        print(f'第{count}次循环')
        llm_resp_message = llm_call(history)
        print('##大模型响应##', llm_resp_message)
        history.append(llm_resp_message)
        response, is_final_flag = react(llm_resp_message.get('content'))
        history.append(init_message_by_role(role='user', content=f"<工具调用结果>: {response}</工具调用结果>"))
        if is_final_flag:
            print(response)
            return
        count += 1
        if count >= 15:
            response = llm_resp_message.get('content')
            print(response)
            return


if __name__ == '__main__':
    # chat('你好')
    chat(
        '我想查询邮件系统 192.168.0.110 这台主机今天1点到1点30分的cpu利用率，并给出echarts折线图的完整html，并进行分析给出分析报告')


# === analysis/multi_series.py ===
# analysis/multi_series.py
import config
import math
import numpy as np
import logging
from utils.time_utils import group_anomaly_times
from detectors.base import DetectionResult

logger = logging.getLogger("anomaly_detection.multi_series")

def analyze_multi_series(series1, series2, align=True):
    """
    对两个时间序列进行对比分析
    
    参数:
        series1: 第一个时间序列
        series2: 第二个时间序列
        align: 是否对齐时间戳
        
    返回:
        dict: 包含分析结果的字典
    """
    # 输入参数验证
    if not series1 or not series2:
        logger.warning("输入时间序列为空")
        return {
            "method_results": [],
            "composite_score": 0,
            "classification": "正常",
            "anomaly_times": [],
            "anomaly_intervals": []
        }
    
    # 导入放在函数内部避免循环引用
    from detectors.residual_comparison import ResidualComparisonDetector
    from detectors.trend_drift_cusum import TrendDriftCUSUMDetector
    from detectors.change_rate import ChangeRateDetector
    from detectors.trend_slope import TrendSlopeDetector
    from analysis.data_alignment import align_series
    
    # 检查两个序列是否有足够的差异
    values1 = [v for _, v in series1]
    values2 = [v for _, v in series2]
    
    if align:
        try:
            series1, series2 = align_series(series1, series2, method="linear", fill_value="extrapolate")
            logger.info("成功对齐两个时间序列")
        except Exception as e:
            logger.error(f"时间序列对齐失败: {e}")
            # 继续使用原始序列
    
    # 计算两个序列的相似度
    try:
        mean_abs_diff = np.mean(np.abs(np.array(values1) - np.array(values2)))
        relative_diff = mean_abs_diff / (np.mean(np.abs(values1)) + 1e-10)
        
        logger.info(f"两序列的相对差异: {relative_diff:.1%}")
        # 如果差异极小，可能不需要详细分析
        if relative_diff < 0.05:  # 小于5%的差异
            logger.info("序列几乎相同，无需详细分析")
            return {
                "method_results": [],
                "composite_score": 0,
                "classification": "正常",
                "anomaly_times": [],
                "anomaly_intervals": []
            }
    except Exception as e:
        logger.warning(f"计算序列差异失败: {e}")
        # 继续分析

    # 加载阈值配置
    thres = config.THRESHOLD_CONFIG
    
    # 执行各个检测方法
    detection_results = []
    
    # 1. 残差对比方法
    try:
        res_residual = ResidualComparisonDetector(
            threshold=thres.get("ResidualComparison", {}).get("threshold", 3.5)
        ).detect(series1, series2)
        detection_results.append(res_residual)
        logger.info(f"残差对比检测到 {len(res_residual.anomalies)} 个异常点")
    except Exception as e:
        logger.error(f"残差对比检测失败: {e}")
    
    # 2. 趋势漂移CUSUM方法
    try:
        res_drift = TrendDriftCUSUMDetector(
            threshold=thres.get("TrendDriftCUSUM", {}).get("drift_threshold", 8.0)
        ).detect(series1, series2)
        detection_results.append(res_drift)
        logger.info(f"趋势漂移检测到 {len(res_drift.intervals)} 个异常区间")
    except Exception as e:
        logger.error(f"趋势漂移检测失败: {e}")
    
    try:
        res_change = ChangeRateDetector(
            threshold=thres.get("ChangeRate", {}).get("threshold", 0.7)
        ).detect(series1, series2)
        detection_results.append(res_change)
        logger.info(f"变化率检测到 {len(res_change.explanation)} 个文本解释")
    except Exception as e:
        logger.error(f"变化率检测失败: {e}")
    
    try:
        res_slope = TrendSlopeDetector(
            threshold=thres.get("TrendSlope", {}).get("slope_threshold", 0.4),
            window=thres.get("TrendSlope", {}).get("window", 5)
        ).detect(series1, series2)
        detection_results.append(res_slope)
        logger.info(f"趋势斜率检测到 {len(res_slope.anomalies)} 个异常点")
    except Exception as e:
        logger.error(f"趋势斜率检测失败: {e}")

    method_results = [
        r for r in detection_results if r is not None
    ]
    
    if not method_results:
        logger.warning("所有检测方法都失败")
        return {
            "method_results": [],
            "composite_score": 0,
            "classification": "正常",
            "anomaly_times": [],
            "anomaly_intervals": []
        }

    total_weight = 0.0
    composite_score = 0.0
    length = max(len(series1), len(series2)) or 1
    
    method_scores = {}

    for res in method_results:
        m_name = res.method
        weight = config.WEIGHTS_MULTI.get(m_name, 0.25) 
        total_weight += weight
        
        if res.visual_type == "none" and res.explanation:
            has_significant_diff = any(("差异较大" in expl or "明显" in expl) 
                                      for expl in res.explanation)
            method_score = 0.4 if has_significant_diff else 0.2 if res.explanation else 0
        else:
            anomaly_count = len(res.anomalies)
            interval_count = len(res.intervals) * 3  
            total_count = anomaly_count + interval_count
            
            if total_count > 0:
                #计算异常比例
                if m_name == "TrendDriftCUSUM":
                    total_duration = 0
                    for start, end in res.intervals:
                        total_duration += (end - start)
                    
                    coverage_ratio = total_duration / (series1[-1][0] - series1[0][0] + 1)
                    if coverage_ratio > 0.5:  
                        method_score = min(0.7, 0.4 + 0.3 * coverage_ratio)
                    else:
                        method_score = 0.3 * coverage_ratio
                else:
                    anomaly_ratio = total_count / length
                    if anomaly_ratio < 0.01:
                        method_score = 0.2 + 0.3 * (anomaly_ratio * 100)
                    else:
                        method_score = min(0.8, 0.2 + 0.3 * np.log10(1 + anomaly_ratio * 100))
            else:
                method_score = 0
        
        #各方法得分
        method_scores[m_name] = method_score
        composite_score += weight * method_score
        logger.info(f"方法 {m_name} 得分: {method_score:.2f}, 权重: {weight}")

    if total_weight > 0:
        composite_score /= total_weight

    methods_with_anomalies = sum(1 for res in method_results 
                              if (len(res.anomalies) > 0 or 
                                  len(res.intervals) > 0 or 
                                  (res.visual_type == "none" and len(res.explanation) > 0)))
    
    if methods_with_anomalies == 1 and len(method_results) > 1:
        logger.info("仅一个方法检测到异常，降低得分")
        composite_score *= 0.8
    
    if "TrendDriftCUSUM" in method_scores and method_scores["TrendDriftCUSUM"] > 0.5:
        other_methods_score = sum(score for name, score in method_scores.items() 
                                if name != "TrendDriftCUSUM") / max(1, len(method_scores) - 1)
        
        if other_methods_score < 0.2:
            logger.warning("TrendDriftCUSUM可能误报，降低总得分")
            composite_score = (composite_score + other_methods_score) / 2

    classification = (
        "高置信度异常" if composite_score >= config.HIGH_ANOMALY_THRESHOLD
        else "轻度异常" if composite_score >= config.MILD_ANOMALY_THRESHOLD
        else "正常"
    )
    
    logger.info(f"综合得分: {composite_score:.2f}, 分类: {classification}")

    #合并所有异常点
    all_anoms = set()
    for r in method_results:
        all_anoms.update(r.anomalies)
    anomaly_list = sorted(all_anoms)
    
    anomaly_ratio = len(anomaly_list) / length if length > 0 else 0
    if anomaly_ratio > 0.25:
        logger.warning(f"异常点比例高达 {anomaly_ratio:.1%}，重新评估分类")
        if methods_with_anomalies < len(method_results) * 0.7:
            if classification == "高置信度异常":
                classification = "轻度异常"
                logger.info("降级为轻度异常")
            elif classification == "轻度异常" and anomaly_ratio > 0.4:
                classification = "正常"
                logger.info("降级为正常")
    
    intervals = group_anomaly_times(anomaly_list)

    return {
        "method_results": method_results,
        "composite_score": composite_score,
        "classification": classification,
        "anomaly_times": anomaly_list,
        "anomaly_intervals": intervals
    }

# === analysis/data_alignment.py ===

import numpy as np
from scipy.interpolate import interp1d

def align_series(series1, series2, method="linear", fill_value="extrapolate"):
    
    #使用SciPy的interp1d进行插值，将series1和series2在相同的时间戳上对齐
    if not series1 or not series2:
        return series1, series2

    s1_sorted = sorted(series1, key=lambda x: x[0])
    s2_sorted = sorted(series2, key=lambda x: x[0])

    t1 = np.array([row[0] for row in s1_sorted], dtype=np.float64)
    v1 = np.array([row[1] for row in s1_sorted], dtype=np.float64)
    t2 = np.array([row[0] for row in s2_sorted], dtype=np.float64)
    v2 = np.array([row[1] for row in s2_sorted], dtype=np.float64)

    all_ts = np.union1d(t1, t2)

    f1 = interp1d(t1, v1, kind=method, fill_value=fill_value, bounds_error=False)
    f2 = interp1d(t2, v2, kind=method, fill_value=fill_value, bounds_error=False)

    new_v1 = f1(all_ts)
    new_v2 = f2(all_ts)

    s1_aligned = [[int(ts), float(val)] for ts, val in zip(all_ts, new_v1)]
    s2_aligned = [[int(ts), float(val)] for ts, val in zip(all_ts, new_v2)]

    return s1_aligned, s2_aligned


# === analysis/__init__.py ===
# analysis/__init__.py
"""
分析模块，包含单序列和多序列时序数据分析功能
"""
from analysis.single_series import analyze_single_series
from analysis.multi_series import analyze_multi_series
from analysis.data_alignment import align_series

# === analysis/single_series.py ===
# analysis/single_series.py
import config
import numpy as np
import logging
from detectors.base import DetectionResult

logger = logging.getLogger("anomaly_detection.single_series")

def analyze_single_series(series):

    if not series:
        logger.warning("输入时间序列为空")
        return {
            "method_results": [],
            "composite_score": 0,
            "classification": "正常",
            "anomaly_times": []
        }
        
    values = [v for _, v in series]
    if len(set(values)) <= 1:
        logger.info("输入时间序列几乎不变，无需检测")
        return {
            "method_results": [],
            "composite_score": 0,
            "classification": "正常",
            "anomaly_times": []
        }
        
    from detectors.zscore import ZScoreDetector
    from detectors.cusum import CUSUMDetector
    
    thres = config.THRESHOLD_CONFIG
    try:
        res_z = ZScoreDetector(
            threshold=thres.get("Z-Score", {}).get("threshold", 3.5)
        ).detect(series)
        logger.info(f"Z-Score 检测到 {len(res_z.anomalies)} 个异常点")
    except Exception as e:
        logger.error(f"Z-Score 检测失败: {e}")
        res_z = DetectionResult(method="Z-Score", description=f"检测失败: {e}")

    try:
        res_cusum = CUSUMDetector(
            drift_threshold=thres.get("CUSUM", {}).get("drift_threshold", 6.0),
            k=thres.get("CUSUM", {}).get("k", 0.7)
        ).detect(series)
        logger.info(f"CUSUM 检测到 {len(res_cusum.anomalies)} 个异常点, {len(res_cusum.intervals)} 个异常区间")
    except Exception as e:
        logger.error(f"CUSUM 检测失败: {e}")
        res_cusum = DetectionResult(method="CUSUM", description=f"检测失败: {e}")

    valid_results = []
    for result in [res_z, res_cusum]:
        if len(result.anomalies) > 0 or len(result.intervals) > 0 or result.visual_type != "none":
            valid_results.append(result)
    
    if not valid_results:
        logger.warning("所有检测方法都未找到异常或失败")
        return {
            "method_results": [res_z, res_cusum],
            "composite_score": 0,
            "classification": "正常",
            "anomaly_times": []
        }
    
    method_results = []
    method_names = set()
    
    for result in [res_z, res_cusum]:
        if result.method in method_names:
            logger.info(f"跳过重复的 {result.method} 结果")
            continue
        method_results.append(result)
        method_names.add(result.method)
    
    total_weight = 0.0
    composite_score = 0.0
    length = len(series)
    
    for res in method_results:
        m_name = res.method
        weight = config.WEIGHTS_SINGLE.get(m_name, 0.3)
        total_weight += weight
        
        anomalies_count = len(res.anomalies)
        intervals_count = len(res.intervals) * 3
        total_count = anomalies_count + intervals_count
        
        if res.visual_type == "none":
            method_score = 0
        elif total_count > 0:
            if total_count / length < 0.01:
                method_score = 0.2 + 0.3 * (total_count / length) * 100
            else:
                ratio = total_count / length
                method_score = min(0.9, 0.2 + 0.3 * np.log10(1 + ratio * 100))
        else:
            method_score = 0
        
        logger.info(f"方法 {m_name} 得分: {method_score:.2f}, 权重: {weight}")
        composite_score += weight * method_score
    
    if total_weight > 0:
        composite_score /= total_weight
    
    methods_with_anomalies = sum(1 for res in method_results 
                              if len(res.anomalies) > 0 or len(res.intervals) > 0)
    if methods_with_anomalies == 1 and len(method_results) > 1:
        logger.info("仅一个方法检测到异常，降低得分")
        composite_score *= 0.8
    
    classification = (
        "高置信度异常" if composite_score >= config.HIGH_ANOMALY_THRESHOLD
        else "轻度异常" if composite_score >= config.MILD_ANOMALY_THRESHOLD
        else "正常"
    )
    
    logger.info(f"综合得分: {composite_score:.2f}, 分类: {classification}")
    
    #合并所有异常点
    all_anomalies = set()
    for r in method_results:
        all_anomalies.update(r.anomalies)

    anomaly_ratio = len(all_anomalies) / length if length > 0 else 0

    if anomaly_ratio > 0.25:
        logger.warning(f"异常点比例高达 {anomaly_ratio:.1%}，重新评估分类")
        if methods_with_anomalies < len(method_results) * 0.7: 
            if classification == "高置信度异常":
                classification = "轻度异常"
                logger.info("降级为轻度异常")
            elif classification == "轻度异常" and anomaly_ratio > 0.4:
                classification = "正常"
                logger.info("降级为正常")
    
    return {
        "method_results": method_results,
        "composite_score": composite_score,
        "classification": classification,
        "anomaly_times": sorted(all_anomalies)
    }

# === detectors/trend_drift_cusum.py ===
# detectors/trend_drift_cusum.py
import numpy as np
from detectors.base import DetectionResult
from utils.time_utils import group_anomaly_times,format_timestamp

class TrendDriftCUSUMDetector:
    def __init__(self, threshold: float = 5.0):
        self.threshold = threshold
    
    def detect(self, series1: list, series2: list) -> DetectionResult:
        if not series1 or not series2 or len(series1) < 10 or len(series2) < 10:
            return DetectionResult(
                method="TrendDriftCUSUM",
                description="数据点不足进行趋势漂移分析(至少需要10个点)",
                visual_type="none"
            )
    
        residuals = []
        timestamps = [t for t, _ in series1]
        values1 = [v for _, v in series1]
        values2 = [v for _, v in series2]
        
        for (_, v1), (_, v2) in zip(series1, series2):
            residuals.append(v1 - v2)
        
        mean_abs_residual = np.mean(np.abs(residuals))
        max_abs_residual = np.max(np.abs(residuals))
        relative_diff = mean_abs_residual / (np.mean(np.abs(values1)) + 1e-10)

        if max_abs_residual < 0.05 and mean_abs_residual < 0.01:
            return DetectionResult(
                method="TrendDriftCUSUM",
                description=f"两序列几乎相同，最大差异仅{max_abs_residual:.3f}，平均差异{mean_abs_residual:.3f}",
                visual_type="none"
            )
        
        if relative_diff < 0.1:
            return DetectionResult(
                method="TrendDriftCUSUM",
                description=f"两序列差异不显著(相对差异{relative_diff:.1%})，无需进行漂移分析",
                visual_type="none"
            )
        
        def smooth_data(data, window=3):
            if len(data) < window:
                return data
            smoothed = np.convolve(data, np.ones(window)/window, mode='same')
            smoothed[:window//2] = data[:window//2]
            smoothed[-window//2:] = data[-window//2:]
            return smoothed
        
        smoothed_residuals = smooth_data(residuals, window=5)

        mean = np.mean(smoothed_residuals)
        std = np.std(smoothed_residuals) 
 
        if std < 1e-10:
            std = 1.0

        norm_residuals = [(r - mean) / std for r in smoothed_residuals]

        control_factor = 1.0
        cum_sum_pos = [0]
        cum_sum_neg = [0]
        
        for r in norm_residuals:
            cum_sum_pos.append(max(0, cum_sum_pos[-1] + r - control_factor))
            cum_sum_neg.append(max(0, cum_sum_neg[-1] - r - control_factor))
        
        cum_sum_pos = cum_sum_pos[1:]
        cum_sum_neg = cum_sum_neg[1:]
        cum_sum = [max(p, n) for p, n in zip(cum_sum_pos, cum_sum_neg)]

        anomalies = []
        scores = []
        consecutive_count = 0
        required_consecutive = 3 #至少需要连续3个点超过阈值
        
        for i, c in enumerate(cum_sum):
            if c > self.threshold:
                consecutive_count += 1
                if consecutive_count >= required_consecutive:
    
                    if consecutive_count == required_consecutive:
                        anomalies.append(timestamps[i - required_consecutive + 1])
                        scores.append(float(c))
                    anomalies.append(timestamps[i])
                    scores.append(float(c))
            else:
                consecutive_count = 0
        
        intervals = group_anomaly_times(anomalies, max_gap=300) #5分钟间隔

        filtered_intervals = []
        explanations = []
        
        for interval in intervals:
            start, end = interval
            duration = end - start
            
            interval_indices = [i for i, ts in enumerate(timestamps) if start <= ts <= end]
            if not interval_indices:
                continue
                
            interval_scores = [cum_sum[i] for i in interval_indices]
            avg_score = np.mean(interval_scores) if interval_scores else 0
            max_score = np.max(interval_scores) if interval_scores else 0
                
 
            if duration >= 300 and avg_score > self.threshold * 1.3 and max_score > self.threshold * 1.5:
                filtered_intervals.append(interval)
                explanations.append(
                    f"区间{format_timestamp(start)}至{format_timestamp(end)}的CUSUM值平均为{avg_score:.1f}，最大值{max_score:.1f}，超过阈值{self.threshold}，表明两序列存在持续趋势差异"
                )
        
        if not filtered_intervals:
            return DetectionResult(
                method="TrendDriftCUSUM",
                description=f"趋势漂移检测未发现明显的持续性异常区段",
                visual_type="none"
            )
        
        #辅助曲线数据
        aux_curve = [(timestamps[i], float(cum_sum[i])) for i in range(len(timestamps))]
        
        return DetectionResult(
            method="TrendDriftCUSUM",
            anomalies=[],
            anomaly_scores=[],
            intervals=filtered_intervals,
            auxiliary_curve=aux_curve,
            description=f"趋势漂移检测发现 {len(filtered_intervals)} 个明显异常区段，相对差异{relative_diff:.1%}",
            visual_type="range",
            explanation=explanations
        )

    

# === detectors/base.py ===
# detectors/base.py
from typing import List, Tuple, Optional, Dict, Any, Union

class DetectionResult:
    def __init__(
        self,
        method: str,
        anomalies: Optional[List[int]] = None,
        anomaly_scores: Optional[List[float]] = None,
        intervals: Optional[List[Tuple[int, int]]] = None,
        auxiliary_curve: Optional[List[Tuple[int, float]]] = None,
        description: str = "",
        visual_type: str = "point",  # point | range | curve | none
        explanation: Optional[List[str]] = None,
    ):
        self.method = method
        self.anomalies = anomalies or []
        self.anomaly_scores = anomaly_scores or []
        self.intervals = intervals or []
        self.auxiliary_curve = auxiliary_curve or []
        self.description = description
        self.visual_type = visual_type
        self.explanation = explanation or []
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "method": self.method,
            "anomalies": self.anomalies,
            "anomaly_scores": self.anomaly_scores,
            "intervals": self.intervals,
            "auxiliary_curve": self.auxiliary_curve,
            "description": self.description,
            "visual_type": self.visual_type,
            "explanation": self.explanation
        }

# === detectors/residual_comparison.py ===
import numpy as np
from detectors.base import DetectionResult

class ResidualComparisonDetector:
    def __init__(self, threshold: float = 3.0):
        self.threshold = threshold

    def detect(self, series1: list[tuple[int, float]], series2: list[tuple[int, float]]) -> DetectionResult:
        residuals = []
        timestamps = [t for t, _ in series1]

        for (_, v1), (_, v2) in zip(series1, series2):
            residuals.append(v1 - v2)

        residuals = np.array(residuals)
        mean = np.mean(residuals)
        std = np.std(residuals)
        z_scores = (residuals - mean) / std

        anomalies = []
        scores = []
        explanations = []

        for i, z in enumerate(z_scores):
            if abs(z) > self.threshold:
                anomalies.append(timestamps[i])
                scores.append(round(abs(z), 3))
                explanations.append(f"残差Z值={z:.2f}，两序列差异大")

        return DetectionResult(
            method="ResidualComparison",
            anomalies=anomalies,
            anomaly_scores=scores,
            description=f"基于残差Z分数检测出 {len(anomalies)} 个异常点（阈值={self.threshold}）",
            visual_type="point",
            explanation=explanations
        )


# === detectors/zscore.py ===
# detectors/zscore.py
import numpy as np
from typing import List, Tuple, Optional
from detectors.base import DetectionResult

class ZScoreDetector:
    def __init__(self, threshold: float = 3.0):

        self.threshold = threshold
    
    def detect(self, series: List[Tuple[int, float]]) -> DetectionResult:

        if not series:
            return DetectionResult(
                method="Z-Score",
                description="无数据进行Z-Score分析",
                visual_type="none"
            )
        

        timestamps = [t for t, _ in series]
        values = np.array([v for _, v in series])

        mean = np.mean(values)
        std = np.std(values) if len(values) > 1 else 1.0
        
        z_scores = (values - mean) / std if std > 0 else np.zeros_like(values)
        
        anomalies = []
        scores = []
        explanations = []
        
        for i, z in enumerate(z_scores):
            if abs(z) > self.threshold:
                anomalies.append(timestamps[i])
                scores.append(float(abs(z)))
                # 解释是高于均值还是低于均值
                direction = "高于" if z > 0 else "低于"
                explanations.append(f"Z-Score={z:.2f}，{direction}均值{abs(z):.2f}个标准差")
        
        return DetectionResult(
            method="Z-Score",
            anomalies=anomalies,
            anomaly_scores=scores,
            description=f"使用Z-Score方法(阈值={self.threshold})检测到{len(anomalies)}个异常点",
            visual_type="point",
            explanation=explanations
        )

# === detectors/__init__.py ===
# detectors/__init__.py
"""
异常检测器模块，包含各种用于时序数据异常检测的算法
"""

# 明确导出检测器类以简化导入
from detectors.zscore import ZScoreDetector
from detectors.cusum import CUSUMDetector
from detectors.residual_comparison import ResidualComparisonDetector
from detectors.trend_drift_cusum import TrendDriftCUSUMDetector
from detectors.change_rate import ChangeRateDetector
from detectors.trend_slope import TrendSlopeDetector
from detectors.base import DetectionResult

# 为向后兼容性导出旧的函数名
from detectors.zscore import ZScoreDetector as detect_zscore
from detectors.cusum import CUSUMDetector as detect_cusum
from detectors.residual_comparison import ResidualComparisonDetector as detect_residual_compare
from detectors.trend_drift_cusum import TrendDriftCUSUMDetector as detect_trend_drift
from detectors.change_rate import ChangeRateDetector as detect_change_rate
from detectors.trend_slope import TrendSlopeDetector as detect_trend_slope

# === detectors/trend_slope.py ===
# detectors/trend_slope.py
import numpy as np
from detectors.base import DetectionResult

class TrendSlopeDetector:
    def __init__(self, window: int = 5, threshold: float = 0.2, slope_threshold: float = None):
        self.window = window
        self.threshold = slope_threshold if slope_threshold is not None else threshold
    
    def detect(self, series1: list, series2: list) -> DetectionResult:
        if not series1 or not series2 or len(series1) < self.window or len(series2) < self.window:
            return DetectionResult(
                method="TrendSlope",
                description="数据点不足进行趋势斜率分析",
                visual_type="none"
            )
            
        def calc_slope(values):
            x = np.arange(len(values))
            A = np.vstack([x, np.ones(len(values))]).T
            m, _ = np.linalg.lstsq(A, values, rcond=None)[0]
            return m

        slopes1, slopes2, timestamps = [], [], []

        for i in range(len(series1) - self.window + 1):
            window1 = [v for _, v in series1[i:i + self.window]]
            window2 = [v for _, v in series2[i:i + self.window]]
            
            try:
                slope1 = calc_slope(window1)
                slope2 = calc_slope(window2)
                slopes1.append(slope1)
                slopes2.append(slope2)
                timestamps.append(series1[i + self.window // 2][0])
            except Exception as e:
                print(f"计算斜率时出错: {e}")
                continue

        if not timestamps:
            return DetectionResult(
                method="TrendSlope",
                description="无法计算有效的趋势斜率",
                visual_type="none"
            )

        slope_diff = np.abs(np.array(slopes1) - np.array(slopes2))
        sorted_indices = np.argsort(-slope_diff)
        
        anomalies = []
        scores = []
        explanations = []
        
        for i in sorted_indices[:min(3, len(sorted_indices))]:
            ts = timestamps[i]
            diff = slope_diff[i]
            if diff > self.threshold:
                anomalies.append(ts)
                scores.append(float(diff))
                explanations.append(f"趋势斜率差值为 {diff:.3f}，高于阈值 {self.threshold}")
        
        return DetectionResult(
            method="TrendSlope",
            anomalies=anomalies,
            anomaly_scores=scores,
            description=f"TrendSlope 检测两个序列在滑动窗口下的局部趋势方向差异，发现 {len(anomalies)} 个异常点",
            visual_type="point",
            explanation=explanations
        )

# === detectors/change_rate.py ===
import numpy as np
from detectors.base import DetectionResult

class ChangeRateDetector:
    def __init__(self, threshold: float = 0.1):
        self.threshold = threshold

    def detect(self, series1: list[tuple[int, float]], series2: list[tuple[int, float]]) -> DetectionResult:
        timestamps = [t for t, _ in series1]
        rate_diffs = []

        for i in range(1, len(series1)):
            delta1 = series1[i][1] - series1[i - 1][1]
            delta2 = series2[i][1] - series2[i - 1][1]
            rate_diff = abs(delta1 - delta2)
            rate_diffs.append((timestamps[i], rate_diff))

        sorted_diff = sorted(rate_diffs, key=lambda x: -x[1])
        top = sorted_diff[:3]
        explanations = [
            f"{ts}: 变化速率差值为 {round(diff, 3)}，差异较大"
            for ts, diff in top
        ]

        return DetectionResult(
            method="ChangeRate",
            description="ChangeRate 用于比较两个序列的局部变化速度，检测出速率差异较大的时间点",
            visual_type="none",
            explanation=explanations
        )


# === detectors/cusum.py ===
# detectors/cusum.py
import numpy as np
from typing import List, Tuple, Optional
from detectors.base import DetectionResult

class CUSUMDetector:
    def __init__(self, drift_threshold: float = 5.0, k: float = 0.5):
        """
        参数:
            drift_threshold: CUSUM阈值
            k: 灵敏度参数，较小的值对小偏移更敏感
        """
        self.drift_threshold = drift_threshold
        self.k = k
    
    def detect(self, series: List[Tuple[int, float]]) -> DetectionResult:
        if not series:
            return DetectionResult(
                method="CUSUM",
                description="无数据进行CUSUM分析",
                visual_type="none"
            )
        
        timestamps = [t for t, _ in series]
        values = np.array([v for _, v in series])
        
        mean = np.mean(values)
        std = np.std(values) if len(values) > 1 else 1.0
        
        cusum_pos = np.zeros(len(values))
        cusum_neg = np.zeros(len(values))
        

        for i in range(1, len(values)):
            cusum_pos[i] = max(0, cusum_pos[i-1] + (values[i] - mean)/std - self.k)
            cusum_neg[i] = max(0, cusum_neg[i-1] - (values[i] - mean)/std - self.k)
        
        cusum_combined = np.maximum(cusum_pos, cusum_neg)
        
        anomalies = []
        scores = []
        for i, c in enumerate(cusum_combined):
            if c > self.drift_threshold:
                anomalies.append(timestamps[i])
                scores.append(float(c))
        
        explanations = [
            f"CUSUM值={scores[i]:.2f}，累计偏移超过阈值({self.drift_threshold})"
            for i in range(len(anomalies))
        ]
        
        from analysis.multi_series import group_anomaly_times
        intervals = group_anomaly_times(anomalies)
        
        cum_curve = [(timestamps[i], float(cusum_combined[i])) for i in range(len(timestamps))]
        
        return DetectionResult(
            method="CUSUM",
            anomalies=anomalies,
            anomaly_scores=scores,
            intervals=intervals,
            auxiliary_curve=cum_curve,
            description=f"CUSUM累积偏移检测到 {len(intervals)} 个异常区段，共 {len(anomalies)} 个高偏移点",
            visual_type="curve",
            explanation=explanations
        )

